{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Environment Setup Complete ---\n",
      "Using device: cuda\n",
      "GPU Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install Libraries & Imports\n",
    "\n",
    "# Install required libraries\n",
    "!pip install transformers torch pandas numpy scikit-learn tqdm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "# Set the device to the A100 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- Environment Setup Complete ---\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration Loaded (Tuning Co-Attention Model) ---\n",
      "Model Dims: 64/32/32\n",
      "Epochs: 50, Dropout: 0.3, LR: 5e-05\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Project Configuration (Co-Attention + Simple Dims + Regularization)\n",
    "\n",
    "class Config:\n",
    "    # --- File Paths ---\n",
    "    SOURCE_DOMAIN_FILE = 'Movies_and_TV.jsonl'\n",
    "    TARGET_DOMAIN_FILE = 'Digital_Music.jsonl'\n",
    "    \n",
    "    # --- Data Processing ---\n",
    "    MIN_REVIEWS_SOURCE = 10\n",
    "    MIN_REVIEWS_TARGET = 5\n",
    "    TEST_SET_SIZE = 0.2\n",
    "    \n",
    "    # --- Model Hyperparameters ---\n",
    "    LLM_MODEL_NAME = 'distilbert-base-uncased'\n",
    "    EMBEDDING_DIM = 768\n",
    "    \n",
    "    # --- Use SIMPLE Dims ---\n",
    "    FEATURE_DIM = 64     # Was 128\n",
    "    SHARED_DIM = 32      # Was 64\n",
    "    SPECIFIC_DIM = 32    # Was 64\n",
    "    TOP_K_REVIEWS = 15   # Keep 15\n",
    "    DROPOUT_RATE = 0.3   # Keep Dropout\n",
    "    \n",
    "    # --- Training Hyperparameters ---\n",
    "    EPOCHS = 50          # Keep 50 epochs\n",
    "    EMBEDDING_BATCH_SIZE = 64\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 5e-5 # Keep smaller LR\n",
    "    WEIGHT_DECAY = 1e-5  # Keep Weight Decay\n",
    "    \n",
    "    # --- Loss Weights (from Paper 1, Sec III-E) ---\n",
    "    L_PRED_COEFF = 1000.0\n",
    "    L_DIFF_COEFF = 0.1\n",
    "    L_CLASS_COEFF = 0.1\n",
    "    L_RECON_COEFF = 0.1\n",
    "    L_IREC_COEFF = 0.1\n",
    "\n",
    "print(\"--- Configuration Loaded (Tuning Co-Attention Model) ---\")\n",
    "print(f\"Model Dims: {Config.FEATURE_DIM}/{Config.SHARED_DIM}/{Config.SPECIFIC_DIM}\")\n",
    "print(f\"Epochs: {Config.EPOCHS}, Dropout: {Config.DROPOUT_RATE}, LR: {Config.LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Data Loading & Splitting (Memory-Efficient) ---\n",
      "Pass 1: Counting reviews for all users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Movies_and_TV.jsonl: 17328314it [01:41, 171451.98it/s]\n",
      "Counting Digital_Music.jsonl: 130434it [00:00, 154909.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for shared, active users...\n",
      "Found 627 shared users after filtering.\n",
      "Pass 2: Loading data for filtered users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Movies_and_TV.jsonl data: 17328314it [01:34, 183308.48it/s]\n",
      "Loading Digital_Music.jsonl data: 130434it [00:00, 169246.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final DataFrames...\n",
      "Training records: 67138\n",
      "Testing records (cold-start): 1161\n",
      "--- Phase 1 Complete (198.87s) ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Phase 1 - Data Loading & Splitting (Memory-Efficient Version)\n",
    "\n",
    "import collections\n",
    "\n",
    "def stream_and_filter_data(config):\n",
    "    \"\"\"\n",
    "    This new function uses a two-pass streaming approach to avoid OOM errors.\n",
    "    \"\"\"\n",
    "    print(\"--- Phase 1: Data Loading & Splitting (Memory-Efficient) ---\")\n",
    "    start_phase1 = time.time()\n",
    "\n",
    "    source_path = config.SOURCE_DOMAIN_FILE\n",
    "    target_path = config.TARGET_DOMAIN_FILE\n",
    "\n",
    "    # --- Pass 1: Get user review counts ---\n",
    "    print(\"Pass 1: Counting reviews for all users...\")\n",
    "    source_user_counts = collections.defaultdict(int)\n",
    "    target_user_counts = collections.defaultdict(int)\n",
    "\n",
    "    # Count reviews in source file\n",
    "    try:\n",
    "        with open(source_path, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=f\"Counting {os.path.basename(source_path)}\"):\n",
    "                try:\n",
    "                    user_id = json.loads(line)['user_id']\n",
    "                    source_user_counts[user_id] += 1\n",
    "                except (json.JSONDecodeError, KeyError):\n",
    "                    pass # Skip malformed lines or lines without user_id\n",
    "\n",
    "        # Count reviews in target file\n",
    "        with open(target_path, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=f\"Counting {os.path.basename(target_path)}\"):\n",
    "                try:\n",
    "                    user_id = json.loads(line)['user_id']\n",
    "                    target_user_counts[user_id] += 1\n",
    "                except (json.JSONDecodeError, KeyError):\n",
    "                    pass\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: File not found. {e}\")\n",
    "        print(\"Please make sure your .jsonl files are in the same directory as this notebook.\")\n",
    "        return None, None\n",
    "\n",
    "    # --- Filter for shared, active users ---\n",
    "    print(\"Filtering for shared, active users...\")\n",
    "    shared_users = set(source_user_counts.keys()).intersection(set(target_user_counts.keys()))\n",
    "    \n",
    "    filtered_shared_users = set()\n",
    "    for user in shared_users:\n",
    "        if (source_user_counts[user] >= config.MIN_REVIEWS_SOURCE and\n",
    "            target_user_counts[user] >= config.MIN_REVIEWS_TARGET):\n",
    "            filtered_shared_users.add(user)\n",
    "            \n",
    "    print(f\"Found {len(filtered_shared_users)} shared users after filtering.\")\n",
    "\n",
    "    if not filtered_shared_users:\n",
    "        print(\"ERROR: No shared users found with the current filters. Stopping.\")\n",
    "        return None, None\n",
    "\n",
    "    # --- Split users for train/test ---\n",
    "    filtered_user_list = list(filtered_shared_users)\n",
    "    train_user_ids, test_user_ids = train_test_split(filtered_user_list, test_size=config.TEST_SET_SIZE, random_state=42)\n",
    "    train_user_ids = set(train_user_ids)\n",
    "    test_user_ids = set(test_user_ids)\n",
    "\n",
    "    # --- Pass 2: Load ONLY the data we need ---\n",
    "    print(\"Pass 2: Loading data for filtered users...\")\n",
    "    source_data = []\n",
    "    target_data = []\n",
    "\n",
    "    # Load source data\n",
    "    with open(source_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=f\"Loading {os.path.basename(source_path)} data\"):\n",
    "            try:\n",
    "                review = json.loads(line)\n",
    "                if review.get('user_id') in filtered_shared_users:\n",
    "                    source_data.append(review)\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                pass\n",
    "    \n",
    "    # Load target data\n",
    "    with open(target_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=f\"Loading {os.path.basename(target_path)} data\"):\n",
    "            try:\n",
    "                review = json.loads(line)\n",
    "                if review.get('user_id') in filtered_shared_users:\n",
    "                    target_data.append(review)\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                pass\n",
    "\n",
    "    # --- Create final DataFrames ---\n",
    "    print(\"Creating final DataFrames...\")\n",
    "    source_df = pd.DataFrame(source_data)[['user_id', 'parent_asin', 'text', 'rating']]\n",
    "    source_df['domain'] = 'source'\n",
    "    \n",
    "    target_df = pd.DataFrame(target_data)[['user_id', 'parent_asin', 'text', 'rating']]\n",
    "    target_df['domain'] = 'target'\n",
    "    \n",
    "    # --- Create final cold-start splits ---\n",
    "    train_df = pd.concat([\n",
    "        source_df[source_df['user_id'].isin(train_user_ids)],\n",
    "        target_df[target_df['user_id'].isin(train_user_ids)],\n",
    "        source_df[source_df['user_id'].isin(test_user_ids)]\n",
    "    ])\n",
    "    test_df = target_df[target_df['user_id'].isin(test_user_ids)]\n",
    "\n",
    "    print(f\"Training records: {len(train_df)}\")\n",
    "    print(f\"Testing records (cold-start): {len(test_df)}\")\n",
    "    print(f\"--- Phase 1 Complete ({time.time() - start_phase1:.2f}s) ---\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# --- Main execution of this cell ---\n",
    "train_df, test_df = stream_and_filter_data(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Encoder Class Defined ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Phase 2 - BPE-LLM Encoder Class\n",
    "\n",
    "class BpeLlmReviewEncoder:\n",
    "    \"\"\"\n",
    "    Encodes review texts into semantic vectors using BPE tokenization\n",
    "    and a pre-trained DistilBERT model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=Config.LLM_MODEL_NAME, batch_size=Config.EMBEDDING_BATCH_SIZE):\n",
    "        print(\"Initializing BPE-LLM Review Encoder...\")\n",
    "        self.device = device\n",
    "        # Load tokenizer and model from Hugging Face, move to GPU, set to eval mode\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        self.model = DistilBertModel.from_pretrained(model_name).to(self.device).eval()\n",
    "        self.batch_size = batch_size\n",
    "        print(f\"Encoder initialized on {self.device} with model {model_name}.\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, review_texts):\n",
    "        \"\"\"\n",
    "        Takes a list of review texts and returns their aggregated embedding vectors.\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process in batches with a progress bar\n",
    "        for i in tqdm(range(0, len(review_texts), self.batch_size), desc=\"Encoding Reviews (A100)\"):\n",
    "            batch_texts = review_texts[i:i+self.batch_size]\n",
    "            \n",
    "            # Tokenize, pad, truncate, and move to GPU\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True, truncation=True, return_tensors='pt', max_length=512\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Get token embeddings\n",
    "            outputs = self.model(**inputs)\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            \n",
    "            # Mean pooling - mask out padding tokens\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "            mean_pooled = sum_embeddings / sum_mask\n",
    "            \n",
    "            # Move to CPU and store\n",
    "            all_embeddings.append(mean_pooled.cpu().numpy())\n",
    "\n",
    "        return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"--- Encoder Class Defined ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 2: Generating LLM Embeddings ---\n",
      "Found 62685 unique reviews to encode.\n",
      "Initializing BPE-LLM Review Encoder...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder initialized on cuda with model distilbert-base-uncased.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Reviews (A100): 100%|██████████| 980/980 [16:02<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished encoding in 963.54 seconds.\n",
      "Created embedding map.\n",
      "Pre-grouping reviews by user and item...\n",
      "--- Phase 2 Complete (963.80s) ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Phase 2 - Embedding Generation (The Heavy Task)\n",
    "\n",
    "print(\"--- Phase 2: Generating LLM Embeddings ---\")\n",
    "start_phase2 = time.time()\n",
    "\n",
    "# 1. Combine all data to find unique reviews\n",
    "full_df = pd.concat([train_df, test_df])\n",
    "# We must handle potential 'None' or non-string values in 'text'\n",
    "full_df = full_df.dropna(subset=['text'])\n",
    "full_df['text'] = full_df['text'].astype(str)\n",
    "unique_reviews_text = full_df['text'].drop_duplicates().tolist()\n",
    "print(f\"Found {len(unique_reviews_text)} unique reviews to encode.\")\n",
    "\n",
    "# 2. Initialize and run the encoder\n",
    "encoder = BpeLlmReviewEncoder()\n",
    "unique_embeddings = encoder.encode(unique_reviews_text)\n",
    "print(f\"Finished encoding in {time.time() - start_phase2:.2f} seconds.\")\n",
    "\n",
    "# 3. Create a simple dictionary to map review text to its embedding\n",
    "embedding_map = {text: emb for text, emb in zip(unique_reviews_text, unique_embeddings)}\n",
    "print(f\"Created embedding map.\")\n",
    "\n",
    "# 4. Pre-process and group reviews by user and item for fast lookup\n",
    "print(\"Pre-grouping reviews by user and item...\")\n",
    "all_reviews_with_embeddings = full_df.copy()\n",
    "all_reviews_with_embeddings['embedding'] = all_reviews_with_embeddings['text'].map(embedding_map)\n",
    "\n",
    "# Drop rows where embedding might be null (if any texts were dropped)\n",
    "all_reviews_with_embeddings = all_reviews_with_embeddings.dropna(subset=['embedding'])\n",
    "\n",
    "# Group reviews by user_id and domain\n",
    "user_reviews_grouped = all_reviews_with_embeddings.groupby(['user_id', 'domain'])['embedding'].apply(list)\n",
    "# Group reviews by item_id (only in target domain)\n",
    "item_reviews_grouped = all_reviews_with_embeddings[\n",
    "    all_reviews_with_embeddings['domain'] == 'target'\n",
    "].groupby('parent_asin')['embedding'].apply(list)\n",
    "\n",
    "# Convert to dictionary for fast lookup in the Dataset class\n",
    "user_reviews_map = user_reviews_grouped.to_dict()\n",
    "item_reviews_map = item_reviews_grouped.to_dict()\n",
    "\n",
    "print(f\"--- Phase 2 Complete ({time.time() - start_phase2:.2f}s) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 3: Dataset & DataLoader Definition ---\n",
      "Created Train Dataloader with 6423 samples.\n",
      "Created Test Dataloader with 1161 samples.\n",
      "--- Phase 3 Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Phase 3 - PyTorch Dataset & DataLoader\n",
    "\n",
    "class RACRecDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for the RACRec-LLM model.\n",
    "    It fetches pre-computed embeddings for users and items.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, user_reviews_map, item_reviews_map, is_test=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.user_reviews_map = user_reviews_map\n",
    "        self.item_reviews_map = item_reviews_map\n",
    "        self.max_reviews = Config.TOP_K_REVIEWS\n",
    "        self.is_test = is_test # To simulate cold-start\n",
    "        self.embedding_dim = Config.EMBEDDING_DIM\n",
    "        self.zero_pad_vector = np.zeros(self.embedding_dim, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def _pad_reviews(self, reviews_list):\n",
    "        \"\"\"Pads or truncates a list of review embeddings to a fixed size.\"\"\"\n",
    "        # Take the most recent K reviews\n",
    "        reviews = reviews_list[-self.max_reviews:]\n",
    "        \n",
    "        # Pad with zero vectors if fewer than K reviews\n",
    "        padded_reviews = reviews + [self.zero_pad_vector] * (self.max_reviews - len(reviews))\n",
    "        \n",
    "        return np.array(padded_reviews, dtype=np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        user_id = row['user_id']\n",
    "        item_id = row['parent_asin']\n",
    "        rating = float(row['rating'])\n",
    "        \n",
    "        # Get user's source (Movies) reviews\n",
    "        user_source_reviews = self.user_reviews_map.get((user_id, 'source'), [])\n",
    "        \n",
    "        # Get user's target (Music) reviews\n",
    "        if self.is_test:\n",
    "            # *** COLD-START SIMULATION ***\n",
    "            # In the test phase, we pretend the user has no target domain history.\n",
    "            user_target_reviews = []\n",
    "        else:\n",
    "            user_target_reviews = self.user_reviews_map.get((user_id, 'target'), [])\n",
    "        \n",
    "        # Get item's target (Music) reviews\n",
    "        item_target_reviews = self.item_reviews_map.get(item_id, [])\n",
    "        \n",
    "        # Pad all review lists to a fixed size\n",
    "        usr_src_pad = self._pad_reviews(user_source_reviews)\n",
    "        usr_tgt_pad = self._pad_reviews(user_target_reviews)\n",
    "        itm_tgt_pad = self._pad_reviews(item_target_reviews)\n",
    "        \n",
    "        return {\n",
    "            'usr_src_reviews': torch.tensor(usr_src_pad, dtype=torch.float32),\n",
    "            'usr_tgt_reviews': torch.tensor(usr_tgt_pad, dtype=torch.float32),\n",
    "            'itm_tgt_reviews': torch.tensor(itm_tgt_pad, dtype=torch.float32),\n",
    "            'rating': torch.tensor(rating, dtype=torch.float32),\n",
    "            'domain_label_source': torch.tensor(0, dtype=torch.long), # 0 for source\n",
    "            'domain_label_target': torch.tensor(1, dtype=torch.long)  # 1 for target\n",
    "        }\n",
    "\n",
    "print(\"--- Phase 3: Dataset & DataLoader Definition ---\")\n",
    "\n",
    "# We only use target domain interactions for training/testing the final prediction\n",
    "train_dataset_df = train_df[train_df['domain'] == 'target'].dropna(subset=['rating'])\n",
    "test_dataset_df = test_df.dropna(subset=['rating'])\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = RACRecDataset(train_dataset_df, user_reviews_map, item_reviews_map, is_test=False)\n",
    "test_dataset = RACRecDataset(test_dataset_df, user_reviews_map, item_reviews_map, is_test=True)\n",
    "\n",
    "# Create DataLoader instances\n",
    "# num_workers=2 is a safe and efficient value for most systems\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Config.TRAIN_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Created Train Dataloader with {len(train_dataset)} samples.\")\n",
    "print(f\"Created Test Dataloader with {len(test_dataset)} samples.\")\n",
    "print(\"--- Phase 3 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 4: Model Architecture Defined [Co-Attention + Regularized] ---\n",
      "Model created with Co-Attention & Dropout and moved to cuda.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Phase 4 - Model Architecture [Co-Attention + Regularized]\n",
    "\n",
    "class ReviewSelection(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the \"Reviews Selection\" mechanism (Paper 1, Sec III-B)\n",
    "    using co-attention to find the K most relevant reviews.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, feature_dim, dropout_rate):\n",
    "        super(ReviewSelection, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # FFN with Dropout\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, feature_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate) # <-- ADDED DROPOUT\n",
    "        )\n",
    "        \n",
    "        # Affinity matrix parameter M (Paper 1, Eq. 3)\n",
    "        self.M = nn.Parameter(torch.randn(feature_dim, feature_dim))\n",
    "\n",
    "    def forward(self, urlist, irlist):\n",
    "        batch_size = urlist.shape[0]\n",
    "        urlist_mask = (urlist.sum(dim=-1) != 0)\n",
    "        irlist_mask = (irlist.sum(dim=-1) != 0)\n",
    "\n",
    "        # F(urlist) and F(irlist)\n",
    "        ur_feat = self.ffn(urlist) # (B, K, F)\n",
    "        ir_feat = self.ffn(irlist) # (B, K, F)\n",
    "        \n",
    "        # Calculate Affinity Matrix A\n",
    "        A = torch.bmm(torch.bmm(ur_feat, self.M.unsqueeze(0).expand(batch_size, -1, -1)), ir_feat.transpose(1, 2))\n",
    "        \n",
    "        # Masking\n",
    "        A_mask = torch.bmm(urlist_mask.float().unsqueeze(2), irlist_mask.float().unsqueeze(1))\n",
    "        A.masked_fill_(A_mask == 0, -1e9)\n",
    "\n",
    "        # Co-attention weights\n",
    "        W_urlist = F.softmax(A.max(dim=2).values, dim=1)\n",
    "        W_irlist = F.softmax(A.max(dim=1).values, dim=1)\n",
    "        W_urlist = W_urlist * urlist_mask.float()\n",
    "        W_irlist = W_irlist * irlist_mask.float()\n",
    "\n",
    "        # Final aggregated vector\n",
    "        uv = torch.bmm(W_urlist.unsqueeze(1), urlist).squeeze(1)\n",
    "        iv = torch.bmm(W_irlist.unsqueeze(1), irlist).squeeze(1)\n",
    "        \n",
    "        return uv, iv\n",
    "\n",
    "\n",
    "class RACRecLLM(nn.Module):\n",
    "    \"\"\"\n",
    "    The complete hybrid model, now with Dropout for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(RACRecLLM, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.review_selector = ReviewSelection(\n",
    "            config.EMBEDDING_DIM, config.FEATURE_DIM, config.DROPOUT_RATE\n",
    "        )\n",
    "        \n",
    "        encoder_input_dim = config.EMBEDDING_DIM\n",
    "        self.dropout = nn.Dropout(config.DROPOUT_RATE) # <-- Main Dropout Layer\n",
    "        \n",
    "        # 2. Migration of User Preference Modules\n",
    "        self.user_shared_encoder = nn.Linear(encoder_input_dim, config.SHARED_DIM)\n",
    "        self.user_source_encoder = nn.Linear(encoder_input_dim, config.SPECIFIC_DIM)\n",
    "        self.user_target_encoder = nn.Linear(encoder_input_dim, config.SPECIFIC_DIM)\n",
    "        \n",
    "        self.user_source_decoder = nn.Linear(config.SHARED_DIM + config.SPECIFIC_DIM, encoder_input_dim)\n",
    "        self.user_target_decoder = nn.Linear(config.SHARED_DIM + config.SPECIFIC_DIM, encoder_input_dim)\n",
    "        \n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(config.SHARED_DIM, 2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # 2. Product Feature Generation Modules\n",
    "        self.product_encoder = nn.Linear(encoder_input_dim, config.SHARED_DIM + config.SPECIFIC_DIM)\n",
    "        self.product_decoder = nn.Linear(config.SHARED_DIM + config.SPECIFIC_DIM, encoder_input_dim)\n",
    "        \n",
    "    def difference_loss(self, vec1, vec2):\n",
    "        return F.cosine_similarity(vec1, vec2).mean()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        usr_src = batch['usr_src_reviews']\n",
    "        usr_tgt = batch['usr_tgt_reviews']\n",
    "        itm_tgt = batch['itm_tgt_reviews']\n",
    "        \n",
    "        # --- 1. Get Aggregated Vectors ---\n",
    "        uv_source, _ = self.review_selector(usr_src, itm_tgt)\n",
    "        uv_target, iv_target = self.review_selector(usr_tgt, itm_tgt)\n",
    "        \n",
    "        # Apply dropout to the aggregated vectors\n",
    "        uv_source = self.dropout(uv_source)\n",
    "        uv_target = self.dropout(uv_target)\n",
    "        iv_target = self.dropout(iv_target)\n",
    "        \n",
    "        # --- 2. User Migration Path ---\n",
    "        sh_pv_source = self.user_shared_encoder(uv_source)\n",
    "        sp_pv_source = self.user_source_encoder(uv_source)\n",
    "        sh_pv_target = self.user_shared_encoder(uv_target)\n",
    "        sp_pv_target = self.user_target_encoder(uv_target)\n",
    "        \n",
    "        # --- 3. Product Feature Path ---\n",
    "        pfv = self.product_encoder(iv_target)\n",
    "        \n",
    "        # --- 4. Final Preference Vectors for Prediction ---\n",
    "        th_pv = torch.where(\n",
    "            (usr_tgt.sum(dim=[1,2]) == 0).unsqueeze(1),\n",
    "            sh_pv_source.detach(),\n",
    "            sh_pv_target\n",
    "        )\n",
    "        user_pref_vec_concat = torch.cat([th_pv, sp_pv_target], dim=1) # (B, 32+32) -> (B, 64)\n",
    "        \n",
    "        # --- 5. Rating Prediction ---\n",
    "        rating_pred = (user_pref_vec_concat * pfv).sum(dim=1)\n",
    "        \n",
    "        # --- 6. Calculate All 5 Losses ---\n",
    "        loss_diff = (self.difference_loss(sh_pv_source, sp_pv_source) + \n",
    "                     self.difference_loss(sh_pv_target, sp_pv_target)) / 2.0\n",
    "        \n",
    "        domain_pred_source = self.domain_classifier(sh_pv_source)\n",
    "        domain_pred_target = self.domain_classifier(sh_pv_target)\n",
    "        loss_class = F.nll_loss(domain_pred_source, batch['domain_label_source']) + \\\n",
    "                     F.nll_loss(domain_pred_target, batch['domain_label_target'])\n",
    "        \n",
    "        recon_source_in = torch.cat([sh_pv_source, sp_pv_source], dim=1)\n",
    "        recon_target_in = torch.cat([sh_pv_target, sp_pv_target], dim=1)\n",
    "        uv_source_recon = self.user_source_decoder(recon_source_in)\n",
    "        uv_target_recon = self.user_target_decoder(recon_target_in)\n",
    "        loss_rec = F.mse_loss(uv_source_recon, uv_source.detach()) + \\\n",
    "                   F.mse_loss(uv_target_recon, uv_target.detach())\n",
    "                   \n",
    "        iv_target_recon = self.product_decoder(pfv)\n",
    "        loss_irec = F.mse_loss(iv_target_recon, iv_target.detach())\n",
    "        \n",
    "        return {\n",
    "            \"rating_pred\": rating_pred,\n",
    "            \"loss_diff\": loss_diff,\n",
    "            \"loss_class\": -loss_class, # Maximize classification loss\n",
    "            \"loss_rec\": loss_rec,\n",
    "            \"loss_irec\": loss_irec\n",
    "        }\n",
    "\n",
    "print(\"--- Phase 4: Model Architecture Defined [Co-Attention + Regularized] ---\")\n",
    "model = RACRecLLM(Config).to(device)\n",
    "print(f\"Model created with Co-Attention & Dropout and moved to {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 5: Training Loop Defined ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Phase 5 - Training Loop\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion_pred, config, epoch):\n",
    "    model.train() # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "    total_pred_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{Config.EPOCHS} [Training]\")\n",
    "    for batch in pbar:\n",
    "        # Move batch to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "        \n",
    "        # --- Calculate all 5 losses ---\n",
    "        loss_pred = criterion_pred(outputs['rating_pred'], batch['rating'])\n",
    "        \n",
    "        # Combine losses with coefficients from Paper 1 (Eq. 17)\n",
    "        total_loss_combined = (\n",
    "            config.L_PRED_COEFF * loss_pred +\n",
    "            config.L_DIFF_COEFF * outputs['loss_diff'] +\n",
    "            config.L_CLASS_COEFF * outputs['loss_class'] +\n",
    "            config.L_RECON_COEFF * outputs['loss_rec'] +\n",
    "            config.L_IREC_COEFF * outputs['loss_irec']\n",
    "        )\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        total_loss_combined.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += total_loss_combined.item()\n",
    "        total_pred_loss += loss_pred.item()\n",
    "        pbar.set_postfix(Loss=f\"{total_loss_combined.item():.4f}\", PredLoss=f\"{loss_pred.item():.4f}\")\n",
    "        \n",
    "    return total_loss / len(loader), total_pred_loss / len(loader)\n",
    "\n",
    "print(\"--- Phase 5: Training Loop Defined ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 5: Evaluation Loop Defined ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Phase 5 - Evaluation Loop (Cold-Start)\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"[Evaluating Cold-Start]\")\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # Move batch to GPU\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass - This will automatically trigger the cold-start logic\n",
    "            # because batch['usr_tgt_reviews'] is all zeros\n",
    "            outputs = model(batch)\n",
    "            \n",
    "            all_preds.extend(outputs['rating_pred'].cpu().numpy())\n",
    "            all_labels.extend(batch['rating'].cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
    "    \n",
    "    # Binarize ratings for AUC calculation (e.g., 4+ is \"good\")\n",
    "    labels_binary = [1 if r >= 4.0 else 0 for r in all_labels]\n",
    "    preds_scores = all_preds # Use raw scores for AUC\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(labels_binary, preds_scores)\n",
    "    except ValueError as e:\n",
    "        # This can happen if all labels in a batch are the same class\n",
    "        print(f\"Could not calculate AUC (likely all labels are one class): {e}\")\n",
    "        auc = 0.5\n",
    "        \n",
    "    return rmse, auc\n",
    "\n",
    "print(\"--- Phase 5: Evaluation Loop Defined ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 6: Starting Main Execution (Tuning Co-Attention Model) ---\n",
      "Starting training for 50 epochs on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 37.94it/s, Loss=1416.7894, PredLoss=1.4169] \n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 43.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 1/50 Summary (Time: 3.12s) ---\n",
      "  Train Loss (Total):      4346.2750\n",
      "  Train Loss (Pred ONLY):  4.3464\n",
      "  Val RMSE (Cold-Start):   4.5278\n",
      "  Val AUC (Cold-Start):    0.5571\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.5571 (RMSE: 4.5278) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.64it/s, Loss=736.4739, PredLoss=0.7366]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 2/50 Summary (Time: 2.31s) ---\n",
      "  Train Loss (Total):      1286.6267\n",
      "  Train Loss (Pred ONLY):  1.2868\n",
      "  Val RMSE (Cold-Start):   4.5147\n",
      "  Val AUC (Cold-Start):    0.6184\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.6184 (RMSE: 4.5147) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.21it/s, Loss=887.4182, PredLoss=0.8876]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 43.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 3/50 Summary (Time: 2.39s) ---\n",
      "  Train Loss (Total):      1015.4816\n",
      "  Train Loss (Pred ONLY):  1.0157\n",
      "  Val RMSE (Cold-Start):   4.5068\n",
      "  Val AUC (Cold-Start):    0.6408\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.6408 (RMSE: 4.5068) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.11it/s, Loss=1058.0238, PredLoss=1.0582]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 4/50 Summary (Time: 2.33s) ---\n",
      "  Train Loss (Total):      915.3833\n",
      "  Train Loss (Pred ONLY):  0.9156\n",
      "  Val RMSE (Cold-Start):   4.4959\n",
      "  Val AUC (Cold-Start):    0.6646\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.6646 (RMSE: 4.4959) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.48it/s, Loss=1463.6113, PredLoss=1.4638]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 36.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 5/50 Summary (Time: 2.46s) ---\n",
      "  Train Loss (Total):      845.2375\n",
      "  Train Loss (Pred ONLY):  0.8454\n",
      "  Val RMSE (Cold-Start):   4.4920\n",
      "  Val AUC (Cold-Start):    0.6770\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.6770 (RMSE: 4.4920) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.89it/s, Loss=345.6053, PredLoss=0.3458]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 6/50 Summary (Time: 2.34s) ---\n",
      "  Train Loss (Total):      806.0281\n",
      "  Train Loss (Pred ONLY):  0.8062\n",
      "  Val RMSE (Cold-Start):   4.4885\n",
      "  Val AUC (Cold-Start):    0.6905\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.6905 (RMSE: 4.4885) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 48.29it/s, Loss=565.7816, PredLoss=0.5660]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 39.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 7/50 Summary (Time: 2.58s) ---\n",
      "  Train Loss (Total):      772.5064\n",
      "  Train Loss (Pred ONLY):  0.7727\n",
      "  Val RMSE (Cold-Start):   4.4796\n",
      "  Val AUC (Cold-Start):    0.6938\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.6938 (RMSE: 4.4796) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 51.90it/s, Loss=726.6689, PredLoss=0.7269]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 46.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 8/50 Summary (Time: 2.37s) ---\n",
      "  Train Loss (Total):      764.7878\n",
      "  Train Loss (Pred ONLY):  0.7650\n",
      "  Val RMSE (Cold-Start):   4.4793\n",
      "  Val AUC (Cold-Start):    0.7067\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.7067 (RMSE: 4.4793) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 51.52it/s, Loss=1527.0459, PredLoss=1.5273]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 46.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 9/50 Summary (Time: 2.38s) ---\n",
      "  Train Loss (Total):      747.6519\n",
      "  Train Loss (Pred ONLY):  0.7479\n",
      "  Val RMSE (Cold-Start):   4.4725\n",
      "  Val AUC (Cold-Start):    0.7090\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.7090 (RMSE: 4.4725) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.60it/s, Loss=1597.9304, PredLoss=1.5982]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 44.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 10/50 Summary (Time: 2.32s) ---\n",
      "  Train Loss (Total):      720.9901\n",
      "  Train Loss (Pred ONLY):  0.7212\n",
      "  Val RMSE (Cold-Start):   4.4656\n",
      "  Val AUC (Cold-Start):    0.7220\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.7220 (RMSE: 4.4656) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.34it/s, Loss=668.5604, PredLoss=0.6688]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 44.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 11/50 Summary (Time: 2.37s) ---\n",
      "  Train Loss (Total):      688.1185\n",
      "  Train Loss (Pred ONLY):  0.6884\n",
      "  Val RMSE (Cold-Start):   4.4706\n",
      "  Val AUC (Cold-Start):    0.7380\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.7380 (RMSE: 4.4706) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 51.26it/s, Loss=459.8079, PredLoss=0.4601]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 43.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 12/50 Summary (Time: 2.42s) ---\n",
      "  Train Loss (Total):      697.3710\n",
      "  Train Loss (Pred ONLY):  0.6976\n",
      "  Val RMSE (Cold-Start):   4.4624\n",
      "  Val AUC (Cold-Start):    0.7412\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.7412 (RMSE: 4.4624) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.27it/s, Loss=811.4734, PredLoss=0.8117]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 46.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 13/50 Summary (Time: 2.32s) ---\n",
      "  Train Loss (Total):      694.0061\n",
      "  Train Loss (Pred ONLY):  0.6943\n",
      "  Val RMSE (Cold-Start):   4.4574\n",
      "  Val AUC (Cold-Start):    0.7335\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 50.08it/s, Loss=798.3762, PredLoss=0.7987]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 43.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 14/50 Summary (Time: 2.46s) ---\n",
      "  Train Loss (Total):      675.5050\n",
      "  Train Loss (Pred ONLY):  0.6758\n",
      "  Val RMSE (Cold-Start):   4.4468\n",
      "  Val AUC (Cold-Start):    0.7353\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.68it/s, Loss=1061.1422, PredLoss=1.0614]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 15/50 Summary (Time: 2.35s) ---\n",
      "  Train Loss (Total):      667.3055\n",
      "  Train Loss (Pred ONLY):  0.6676\n",
      "  Val RMSE (Cold-Start):   4.4522\n",
      "  Val AUC (Cold-Start):    0.7457\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.7457 (RMSE: 4.4522) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 51.74it/s, Loss=369.2327, PredLoss=0.3695]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 16/50 Summary (Time: 2.38s) ---\n",
      "  Train Loss (Total):      662.1397\n",
      "  Train Loss (Pred ONLY):  0.6624\n",
      "  Val RMSE (Cold-Start):   4.4441\n",
      "  Val AUC (Cold-Start):    0.7460\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.7460 (RMSE: 4.4441) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.54it/s, Loss=571.8721, PredLoss=0.5722]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 44.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 17/50 Summary (Time: 2.32s) ---\n",
      "  Train Loss (Total):      646.3655\n",
      "  Train Loss (Pred ONLY):  0.6467\n",
      "  Val RMSE (Cold-Start):   4.4394\n",
      "  Val AUC (Cold-Start):    0.7349\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 50.84it/s, Loss=714.0003, PredLoss=0.7143]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 18/50 Summary (Time: 2.42s) ---\n",
      "  Train Loss (Total):      644.2025\n",
      "  Train Loss (Pred ONLY):  0.6445\n",
      "  Val RMSE (Cold-Start):   4.4384\n",
      "  Val AUC (Cold-Start):    0.7506\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.7506 (RMSE: 4.4384) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 50.98it/s, Loss=556.6489, PredLoss=0.5570]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 47.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 19/50 Summary (Time: 2.39s) ---\n",
      "  Train Loss (Total):      641.2500\n",
      "  Train Loss (Pred ONLY):  0.6416\n",
      "  Val RMSE (Cold-Start):   4.4348\n",
      "  Val AUC (Cold-Start):    0.7505\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 50.92it/s, Loss=362.4827, PredLoss=0.3628]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 44.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 20/50 Summary (Time: 2.42s) ---\n",
      "  Train Loss (Total):      631.9885\n",
      "  Train Loss (Pred ONLY):  0.6323\n",
      "  Val RMSE (Cold-Start):   4.4318\n",
      "  Val AUC (Cold-Start):    0.7454\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.79it/s, Loss=828.6212, PredLoss=0.8290]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 42.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 21/50 Summary (Time: 2.33s) ---\n",
      "  Train Loss (Total):      623.9049\n",
      "  Train Loss (Pred ONLY):  0.6242\n",
      "  Val RMSE (Cold-Start):   4.4249\n",
      "  Val AUC (Cold-Start):    0.7372\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.91it/s, Loss=493.9437, PredLoss=0.4943]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 44.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 22/50 Summary (Time: 2.35s) ---\n",
      "  Train Loss (Total):      615.4712\n",
      "  Train Loss (Pred ONLY):  0.6158\n",
      "  Val RMSE (Cold-Start):   4.4226\n",
      "  Val AUC (Cold-Start):    0.7511\n",
      "==================================================\n",
      "\n",
      "  *** New best model saved with AUC: 0.7511 (RMSE: 4.4226) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.33it/s, Loss=664.8723, PredLoss=0.6652]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 44.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 23/50 Summary (Time: 2.37s) ---\n",
      "  Train Loss (Total):      620.8366\n",
      "  Train Loss (Pred ONLY):  0.6212\n",
      "  Val RMSE (Cold-Start):   4.4139\n",
      "  Val AUC (Cold-Start):    0.7397\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 50.83it/s, Loss=408.9024, PredLoss=0.4093]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 24/50 Summary (Time: 2.42s) ---\n",
      "  Train Loss (Total):      606.5585\n",
      "  Train Loss (Pred ONLY):  0.6069\n",
      "  Val RMSE (Cold-Start):   4.4073\n",
      "  Val AUC (Cold-Start):    0.7416\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 51.00it/s, Loss=651.9542, PredLoss=0.6523]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 47.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 25/50 Summary (Time: 2.39s) ---\n",
      "  Train Loss (Total):      613.5620\n",
      "  Train Loss (Pred ONLY):  0.6139\n",
      "  Val RMSE (Cold-Start):   4.4115\n",
      "  Val AUC (Cold-Start):    0.7406\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.26it/s, Loss=1252.8799, PredLoss=1.2533]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 26/50 Summary (Time: 2.33s) ---\n",
      "  Train Loss (Total):      612.7280\n",
      "  Train Loss (Pred ONLY):  0.6131\n",
      "  Val RMSE (Cold-Start):   4.4050\n",
      "  Val AUC (Cold-Start):    0.7343\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.20it/s, Loss=669.5752, PredLoss=0.6700]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 46.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 27/50 Summary (Time: 2.32s) ---\n",
      "  Train Loss (Total):      600.8172\n",
      "  Train Loss (Pred ONLY):  0.6012\n",
      "  Val RMSE (Cold-Start):   4.4046\n",
      "  Val AUC (Cold-Start):    0.7491\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.10it/s, Loss=1171.5205, PredLoss=1.1719]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 43.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 28/50 Summary (Time: 2.35s) ---\n",
      "  Train Loss (Total):      590.9827\n",
      "  Train Loss (Pred ONLY):  0.5914\n",
      "  Val RMSE (Cold-Start):   4.4014\n",
      "  Val AUC (Cold-Start):    0.7416\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.71it/s, Loss=796.4009, PredLoss=0.7968]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 29/50 Summary (Time: 2.31s) ---\n",
      "  Train Loss (Total):      594.9347\n",
      "  Train Loss (Pred ONLY):  0.5953\n",
      "  Val RMSE (Cold-Start):   4.4013\n",
      "  Val AUC (Cold-Start):    0.7476\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 51.06it/s, Loss=330.0175, PredLoss=0.3304]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 46.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 30/50 Summary (Time: 2.40s) ---\n",
      "  Train Loss (Total):      584.8435\n",
      "  Train Loss (Pred ONLY):  0.5853\n",
      "  Val RMSE (Cold-Start):   4.3984\n",
      "  Val AUC (Cold-Start):    0.7459\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.95it/s, Loss=784.5187, PredLoss=0.7849]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 43.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 31/50 Summary (Time: 2.36s) ---\n",
      "  Train Loss (Total):      587.5420\n",
      "  Train Loss (Pred ONLY):  0.5880\n",
      "  Val RMSE (Cold-Start):   4.3948\n",
      "  Val AUC (Cold-Start):    0.7368\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.63it/s, Loss=375.1664, PredLoss=0.3756]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 41.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 32/50 Summary (Time: 2.39s) ---\n",
      "  Train Loss (Total):      579.9597\n",
      "  Train Loss (Pred ONLY):  0.5804\n",
      "  Val RMSE (Cold-Start):   4.3884\n",
      "  Val AUC (Cold-Start):    0.7432\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 49.31it/s, Loss=789.5105, PredLoss=0.7899]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 47.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 33/50 Summary (Time: 2.46s) ---\n",
      "  Train Loss (Total):      570.9099\n",
      "  Train Loss (Pred ONLY):  0.5713\n",
      "  Val RMSE (Cold-Start):   4.3835\n",
      "  Val AUC (Cold-Start):    0.7488\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.51it/s, Loss=371.7736, PredLoss=0.3722]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 34/50 Summary (Time: 2.32s) ---\n",
      "  Train Loss (Total):      574.4719\n",
      "  Train Loss (Pred ONLY):  0.5749\n",
      "  Val RMSE (Cold-Start):   4.3758\n",
      "  Val AUC (Cold-Start):    0.7336\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 47.06it/s, Loss=390.5295, PredLoss=0.3910]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 43.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 35/50 Summary (Time: 2.59s) ---\n",
      "  Train Loss (Total):      579.7705\n",
      "  Train Loss (Pred ONLY):  0.5802\n",
      "  Val RMSE (Cold-Start):   4.3658\n",
      "  Val AUC (Cold-Start):    0.7375\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 51.88it/s, Loss=598.0748, PredLoss=0.5985]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 42.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 36/50 Summary (Time: 2.40s) ---\n",
      "  Train Loss (Total):      565.9414\n",
      "  Train Loss (Pred ONLY):  0.5664\n",
      "  Val RMSE (Cold-Start):   4.3667\n",
      "  Val AUC (Cold-Start):    0.7447\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.38it/s, Loss=1006.1530, PredLoss=1.0066]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 42.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 37/50 Summary (Time: 2.38s) ---\n",
      "  Train Loss (Total):      564.4192\n",
      "  Train Loss (Pred ONLY):  0.5649\n",
      "  Val RMSE (Cold-Start):   4.3639\n",
      "  Val AUC (Cold-Start):    0.7348\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 49.00it/s, Loss=542.2682, PredLoss=0.5428]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 42.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 38/50 Summary (Time: 2.52s) ---\n",
      "  Train Loss (Total):      542.9745\n",
      "  Train Loss (Pred ONLY):  0.5435\n",
      "  Val RMSE (Cold-Start):   4.3608\n",
      "  Val AUC (Cold-Start):    0.7320\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 51.88it/s, Loss=286.8896, PredLoss=0.2874]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 46.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 39/50 Summary (Time: 2.36s) ---\n",
      "  Train Loss (Total):      565.0987\n",
      "  Train Loss (Pred ONLY):  0.5656\n",
      "  Val RMSE (Cold-Start):   4.3556\n",
      "  Val AUC (Cold-Start):    0.7326\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 40.00it/s, Loss=240.1933, PredLoss=0.2407]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 29.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 40/50 Summary (Time: 3.18s) ---\n",
      "  Train Loss (Total):      540.8688\n",
      "  Train Loss (Pred ONLY):  0.5414\n",
      "  Val RMSE (Cold-Start):   4.3501\n",
      "  Val AUC (Cold-Start):    0.7287\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 48.89it/s, Loss=474.9492, PredLoss=0.4755]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 42.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 41/50 Summary (Time: 2.52s) ---\n",
      "  Train Loss (Total):      547.7406\n",
      "  Train Loss (Pred ONLY):  0.5483\n",
      "  Val RMSE (Cold-Start):   4.3477\n",
      "  Val AUC (Cold-Start):    0.7321\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.28it/s, Loss=348.8628, PredLoss=0.3494]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 41.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 42/50 Summary (Time: 2.40s) ---\n",
      "  Train Loss (Total):      542.3779\n",
      "  Train Loss (Pred ONLY):  0.5429\n",
      "  Val RMSE (Cold-Start):   4.3419\n",
      "  Val AUC (Cold-Start):    0.7335\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 47.72it/s, Loss=790.7914, PredLoss=0.7914]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 43/50 Summary (Time: 2.55s) ---\n",
      "  Train Loss (Total):      546.6583\n",
      "  Train Loss (Pred ONLY):  0.5472\n",
      "  Val RMSE (Cold-Start):   4.3427\n",
      "  Val AUC (Cold-Start):    0.7298\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 53.46it/s, Loss=553.7347, PredLoss=0.5543]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 44.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 44/50 Summary (Time: 2.33s) ---\n",
      "  Train Loss (Total):      538.1818\n",
      "  Train Loss (Pred ONLY):  0.5387\n",
      "  Val RMSE (Cold-Start):   4.3426\n",
      "  Val AUC (Cold-Start):    0.7272\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 47.26it/s, Loss=386.2357, PredLoss=0.3868]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 41.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 45/50 Summary (Time: 2.60s) ---\n",
      "  Train Loss (Total):      540.9830\n",
      "  Train Loss (Pred ONLY):  0.5415\n",
      "  Val RMSE (Cold-Start):   4.3401\n",
      "  Val AUC (Cold-Start):    0.7383\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 52.86it/s, Loss=540.4496, PredLoss=0.5410]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 46/50 Summary (Time: 2.34s) ---\n",
      "  Train Loss (Total):      534.1439\n",
      "  Train Loss (Pred ONLY):  0.5347\n",
      "  Val RMSE (Cold-Start):   4.3286\n",
      "  Val AUC (Cold-Start):    0.7232\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 50.87it/s, Loss=597.9464, PredLoss=0.5985]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 47/50 Summary (Time: 2.41s) ---\n",
      "  Train Loss (Total):      532.5452\n",
      "  Train Loss (Pred ONLY):  0.5331\n",
      "  Val RMSE (Cold-Start):   4.3330\n",
      "  Val AUC (Cold-Start):    0.7301\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 49.57it/s, Loss=275.9243, PredLoss=0.2765]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 42.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 48/50 Summary (Time: 2.49s) ---\n",
      "  Train Loss (Total):      531.1182\n",
      "  Train Loss (Pred ONLY):  0.5317\n",
      "  Val RMSE (Cold-Start):   4.3301\n",
      "  Val AUC (Cold-Start):    0.7362\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 [Training]: 100%|██████████| 101/101 [00:01<00:00, 50.98it/s, Loss=346.9146, PredLoss=0.3475]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 45.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 49/50 Summary (Time: 2.41s) ---\n",
      "  Train Loss (Total):      517.3735\n",
      "  Train Loss (Pred ONLY):  0.5180\n",
      "  Val RMSE (Cold-Start):   4.3315\n",
      "  Val AUC (Cold-Start):    0.7390\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 [Training]: 100%|██████████| 101/101 [00:02<00:00, 48.91it/s, Loss=361.5952, PredLoss=0.3622]\n",
      "[Evaluating Cold-Start]: 100%|██████████| 19/19 [00:00<00:00, 40.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Epoch 50/50 Summary (Time: 2.55s) ---\n",
      "  Train Loss (Total):      510.8736\n",
      "  Train Loss (Pred ONLY):  0.5115\n",
      "  Val RMSE (Cold-Start):   4.3299\n",
      "  Val AUC (Cold-Start):    0.7379\n",
      "==================================================\n",
      "\n",
      "--- PROJECT COMPLETE ---\n",
      "Total execution time: 2.03 minutes\n",
      "Best cold-start AUC achieved: 0.7511 (with RMSE: 4.4226)\n",
      "Final model saved to 'best_model_co-attention_tuned.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Phase 6 - Main Execution (Tuning the Co-Attention Model)\n",
    "\n",
    "print(\"--- Phase 6: Starting Main Execution (Tuning Co-Attention Model) ---\")\n",
    "\n",
    "model = RACRecLLM(Config).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n",
    "criterion_pred = nn.MSELoss()\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_auc = 0.0\n",
    "total_start_time = time.time()\n",
    "\n",
    "print(f\"Starting training for {Config.EPOCHS} epochs on {device}...\")\n",
    "\n",
    "for epoch in range(Config.EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # --- Train ---\n",
    "    train_loss, train_pred_loss = train_one_epoch(model, train_loader, optimizer, criterion_pred, Config, epoch)\n",
    "    \n",
    "    # --- Evaluate ---\n",
    "    val_rmse, val_auc = evaluate_model(model, test_loader)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"--- Epoch {epoch+1}/{Config.EPOCHS} Summary (Time: {epoch_time:.2f}s) ---\")\n",
    "    print(f\"  Train Loss (Total):      {train_loss:.4f}\")\n",
    "    print(f\"  Train Loss (Pred ONLY):  {train_pred_loss:.4f}\")\n",
    "    print(f\"  Val RMSE (Cold-Start):   {val_rmse:.4f}\")\n",
    "    print(f\"  Val AUC (Cold-Start):    {val_auc:.4f}\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Save the model if it's the best *AUC*\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        best_rmse = val_rmse\n",
    "        torch.save(model.state_dict(), 'best_model_co-attention_tuned.pth')\n",
    "        print(f\"  *** New best model saved with AUC: {best_auc:.4f} (RMSE: {best_rmse:.4f}) ***\\n\")\n",
    "\n",
    "total_end_time = time.time()\n",
    "print(\"--- PROJECT COMPLETE ---\")\n",
    "print(f\"Total execution time: {(total_end_time - total_start_time) / 60:.2f} minutes\")\n",
    "print(f\"Best cold-start AUC achieved: {best_auc:.4f} (with RMSE: {best_rmse:.4f})\")\n",
    "print(\"Final model saved to 'best_model_co-attention_tuned.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading all necessary components for the interactive demo... ---\n",
      "BPE-LLM Encoder is ready.\n",
      "Configuration from the winning run is loaded.\n",
      "\n",
      "Successfully loaded trained model from 'best_model_simple_tuned.pth'!\n",
      "The interactive demo is ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Load Your Trained Model and Encoder\n",
    "\n",
    "print(\"--- Loading all necessary components for the interactive demo... ---\")\n",
    "\n",
    "# 1. We must redefine the model architecture classes exactly as before\n",
    "# This is from your WINNING run (Simple + Dropout)\n",
    "\n",
    "class RACRecLLM(nn.Module):\n",
    "    \"\"\"\n",
    "    This is our original, simple, mean-pooling model.\n",
    "    We are now adding Dropout for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(RACRecLLM, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        encoder_input_dim = config.EMBEDDING_DIM\n",
    "        self.dropout = nn.Dropout(config.DROPOUT_RATE) # <-- Dropout is included\n",
    "        \n",
    "        # 1. Migration of User Preference Modules\n",
    "        self.user_shared_encoder = nn.Linear(encoder_input_dim, config.SHARED_DIM)\n",
    "        self.user_source_encoder = nn.Linear(encoder_input_dim, config.SPECIFIC_DIM)\n",
    "        self.user_target_encoder = nn.Linear(encoder_input_dim, config.SPECIFIC_DIM)\n",
    "        \n",
    "        self.user_source_decoder = nn.Linear(config.SHARED_DIM + config.SPECIFIC_DIM, encoder_input_dim)\n",
    "        self.user_target_decoder = nn.Linear(config.SHARED_DIM + config.SPECIFIC_DIM, encoder_input_dim)\n",
    "        \n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(config.SHARED_DIM, 2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # 2. Product Feature Generation Modules\n",
    "        self.product_encoder = nn.Linear(encoder_input_dim, config.SHARED_DIM + config.SPECIFIC_DIM)\n",
    "        self.product_decoder = nn.Linear(config.SHARED_DIM + config.SPECIFIC_DIM, encoder_input_dim)\n",
    "        \n",
    "    def difference_loss(self, vec1, vec2):\n",
    "        return F.cosine_similarity(vec1, vec2).mean()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        usr_src = batch['usr_src_reviews'] # (B, K, E)\n",
    "        usr_tgt = batch['usr_tgt_reviews'] # (B, K, E)\n",
    "        itm_tgt = batch['itm_tgt_reviews'] # (B, K, E)\n",
    "        \n",
    "        uv_source = torch.sum(usr_src, dim=1) / torch.clamp((usr_src.sum(dim=-1) != 0).sum(dim=1).unsqueeze(1), min=1)\n",
    "        uv_target = torch.sum(usr_tgt, dim=1) / torch.clamp((usr_tgt.sum(dim=-1) != 0).sum(dim=1).unsqueeze(1), min=1)\n",
    "        iv_target = torch.sum(itm_tgt, dim=1) / torch.clamp((itm_tgt.sum(dim=-1) != 0).sum(dim=1).unsqueeze(1), min=1)\n",
    "        \n",
    "        # Apply dropout (this is automatically disabled by model.eval())\n",
    "        uv_source = self.dropout(uv_source)\n",
    "        uv_target = self.dropout(uv_target)\n",
    "        iv_target = self.dropout(iv_target)\n",
    "        \n",
    "        sh_pv_source = self.user_shared_encoder(uv_source)\n",
    "        sp_pv_source = self.user_source_encoder(uv_source)\n",
    "        sh_pv_target = self.user_shared_encoder(uv_target)\n",
    "        sp_pv_target = self.user_target_encoder(uv_target)\n",
    "        pfv = self.product_encoder(iv_target)\n",
    "        \n",
    "        th_pv = torch.where(\n",
    "            (usr_tgt.sum(dim=[1,2]) == 0).unsqueeze(1),\n",
    "            sh_pv_source.detach(),\n",
    "            sh_pv_target\n",
    "        )\n",
    "        user_pref_vec_concat = torch.cat([th_pv, sp_pv_target], dim=1)\n",
    "        rating_pred = (user_pref_vec_concat * pfv).sum(dim=1)\n",
    "        \n",
    "        # For inference, we only need the rating prediction\n",
    "        return {\"rating_pred\": rating_pred}\n",
    "\n",
    "\n",
    "# 2. Load the BPE-LLM Encoder (from Cell 4)\n",
    "try:\n",
    "    if 'encoder' not in globals() or not isinstance(encoder, BpeLlmReviewEncoder):\n",
    "        print(\"Re-defining BpeLlmReviewEncoder class...\")\n",
    "        class BpeLlmReviewEncoder:\n",
    "            def __init__(self, model_name='distilbert-base-uncased', batch_size=64):\n",
    "                print(\"Initializing BPE-LLM Review Encoder...\")\n",
    "                self.device = device\n",
    "                self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "                self.model = DistilBertModel.from_pretrained(model_name).to(self.device).eval()\n",
    "                self.batch_size = batch_size\n",
    "                print(f\"Encoder initialized on {self.device} with model {model_name}.\")\n",
    "\n",
    "            @torch.no_grad()\n",
    "            def encode(self, review_texts):\n",
    "                all_embeddings = []\n",
    "                for i in range(0, len(review_texts), self.batch_size):\n",
    "                    batch_texts = review_texts[i:i+self.batch_size]\n",
    "                    inputs = self.tokenizer(\n",
    "                        batch_texts, padding=True, truncation=True, return_tensors='pt', max_length=512\n",
    "                    ).to(self.device)\n",
    "                    outputs = self.model(**inputs)\n",
    "                    token_embeddings = outputs.last_hidden_state\n",
    "                    attention_mask = inputs['attention_mask']\n",
    "                    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                    sum_embeddings = torch.sum(token_embeddings * mask_expanded, 1)\n",
    "                    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "                    mean_pooled = sum_embeddings / sum_mask\n",
    "                    all_embeddings.append(mean_pooled.cpu().numpy())\n",
    "                return np.vstack(all_embeddings)\n",
    "        \n",
    "        encoder = BpeLlmReviewEncoder()\n",
    "    print(\"BPE-LLM Encoder is ready.\")\n",
    "\n",
    "    # 3. Load the Config (from your WINNING run)\n",
    "    class Config:\n",
    "        EMBEDDING_DIM = 768\n",
    "        FEATURE_DIM = 64\n",
    "        SHARED_DIM = 32\n",
    "        SPECIFIC_DIM = 32\n",
    "        TOP_K_REVIEWS = 10\n",
    "        DROPOUT_RATE = 0.3\n",
    "        EMBEDDING_BATCH_SIZE = 64\n",
    "    \n",
    "    print(\"Configuration from the winning run is loaded.\")\n",
    "    \n",
    "    # 4. Initialize the model and load the saved weights\n",
    "    inference_model = RACRecLLM(Config).to(device)\n",
    "    \n",
    "    # --- This is your best model file from Run 4 ---\n",
    "    model_path = 'best_model_simple_tuned.pth' \n",
    "    \n",
    "    inference_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    inference_model.eval() # Set model to evaluation mode (turns off dropout)\n",
    "\n",
    "    print(f\"\\nSuccessfully loaded trained model from '{model_path}'!\")\n",
    "    print(\"The interactive demo is ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please make sure all previous cells (especially 2, 4, 7 from Run 4) have been defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction functions are ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Define Helper and Prediction Functions\n",
    "\n",
    "def _pad_reviews(reviews_list, max_reviews, emb_dim, zero_pad_vector):\n",
    "    \"\"\"Pads or truncates a list of review embeddings to a fixed size.\"\"\"\n",
    "    reviews = reviews_list[-max_reviews:]\n",
    "    padded_reviews = reviews + [zero_pad_vector] * (max_reviews - len(reviews))\n",
    "    return np.array(padded_reviews, dtype=np.float32)\n",
    "\n",
    "def predict_cold_start_rating(user_movie_reviews, item_music_reviews, model, encoder, config):\n",
    "    \"\"\"\n",
    "    Predicts a rating for a single item, given a new user's source history.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Process and Encode Reviews ---\n",
    "    user_source_embs = list(encoder.encode(user_movie_reviews))\n",
    "    item_target_embs = list(encoder.encode(item_music_reviews))\n",
    "    \n",
    "    # --- 2. Prepare the Batch ---\n",
    "    zero_pad = np.zeros(config.EMBEDDING_DIM, dtype=np.float32)\n",
    "    \n",
    "    usr_src_pad = _pad_reviews(user_source_embs, config.TOP_K_REVIEWS, config.EMBEDDING_DIM, zero_pad)\n",
    "    itm_tgt_pad = _pad_reviews(item_target_embs, config.TOP_K_REVIEWS, config.EMBEDDING_DIM, zero_pad)\n",
    "    \n",
    "    # *** THIS IS THE KEY COLD-START STEP ***\n",
    "    # Create an all-zero tensor for the user's target (music) history\n",
    "    usr_tgt_pad = np.array(\n",
    "        [zero_pad] * config.TOP_K_REVIEWS, \n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # --- 3. Format for the Model ---\n",
    "    # Create a batch of size 1\n",
    "    batch = {\n",
    "        'usr_src_reviews': torch.tensor(usr_src_pad, dtype=torch.float32).unsqueeze(0).to(device),\n",
    "        'usr_tgt_reviews': torch.tensor(usr_tgt_pad, dtype=torch.float32).unsqueeze(0).to(device),\n",
    "        'itm_tgt_reviews': torch.tensor(itm_tgt_pad, dtype=torch.float32).unsqueeze(0).to(device),\n",
    "    }\n",
    "\n",
    "    # --- 4. Get Prediction ---\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch)\n",
    "        predicted_rating = outputs['rating_pred'].item()\n",
    "        \n",
    "    return predicted_rating\n",
    "\n",
    "print(\"Prediction functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading all components for the interactive demo on cuda ---\n",
      "Re-defining BpeLlmReviewEncoder class...\n",
      "Initializing BPE-LLM Review Encoder...\n",
      "Encoder initialized on cuda with model distilbert-base-uncased.\n",
      "BPE-LLM Encoder is ready.\n",
      "Configuration from the winning run is loaded.\n",
      "\n",
      "Successfully loaded trained model from 'best_model_simple_tuned.pth'!\n",
      "The recommendation engine is ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Load Your Trained Model and Encoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# --- FIX: Define device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- Loading all components for the interactive demo on {device} ---\")\n",
    "\n",
    "# 1. We must redefine the model architecture classes exactly as before\n",
    "# This is from your WINNING run (Simple + Dropout)\n",
    "\n",
    "class RACRecLLM(nn.Module):\n",
    "    \"\"\"\n",
    "    This is our original, simple, mean-pooling model.\n",
    "    We are now adding Dropout for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(RACRecLLM, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        encoder_input_dim = config.EMBEDDING_DIM\n",
    "        self.dropout = nn.Dropout(config.DROPOUT_RATE) # <-- Dropout is included\n",
    "        \n",
    "        # 1. Migration of User Preference Modules\n",
    "        self.user_shared_encoder = nn.Linear(encoder_input_dim, config.SHARED_DIM)\n",
    "        self.user_source_encoder = nn.Linear(encoder_input_dim, config.SPECIFIC_DIM)\n",
    "        self.user_target_encoder = nn.Linear(encoder_input_dim, config.SPECIFIC_DIM)\n",
    "        \n",
    "        self.user_source_decoder = nn.Linear(config.SHARED_DIM + config.SPECIFIC_DIM, encoder_input_dim)\n",
    "        self.user_target_decoder = nn.Linear(config.SHARED_DIM + config.SPECIFIC_DIM, encoder_input_dim)\n",
    "        \n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(config.SHARED_DIM, 2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # 2. Product Feature Generation Modules\n",
    "        self.product_encoder = nn.Linear(encoder_input_dim, config.SHARED_DIM + config.SPECIFIC_DIM)\n",
    "        self.product_decoder = nn.Linear(config.SHARED_DIM + config.SPECIFIC_DIM, encoder_input_dim)\n",
    "        \n",
    "    def difference_loss(self, vec1, vec2):\n",
    "        return F.cosine_similarity(vec1, vec2).mean()\n",
    "\n",
    "    def forward_for_recommendation(self, uv_source, uv_target, iv_target):\n",
    "        \"\"\"\n",
    "        A modified forward pass for real-time recommendation.\n",
    "        We feed in the aggregated vectors directly.\n",
    "        \"\"\"\n",
    "        # Apply dropout (this is automatically disabled by model.eval())\n",
    "        uv_source = self.dropout(uv_source)\n",
    "        uv_target = self.dropout(uv_target)\n",
    "        iv_target = self.dropout(iv_target)\n",
    "        \n",
    "        # --- 2. User Migration Path ---\n",
    "        sh_pv_source = self.user_shared_encoder(uv_source)\n",
    "        sp_pv_source = self.user_source_encoder(uv_source)\n",
    "        sh_pv_target = self.user_shared_encoder(uv_target)\n",
    "        sp_pv_target = self.user_target_encoder(uv_target)\n",
    "        \n",
    "        # --- 3. Product Feature Path ---\n",
    "        pfv = self.product_encoder(iv_target) # Product Feature Vector\n",
    "        \n",
    "        # --- 4. Final Preference Vectors for Prediction ---\n",
    "        # This is the CORE cold-start logic\n",
    "        th_pv = sh_pv_source.detach() # We detach, as we are in cold-start\n",
    "        \n",
    "        user_pref_vec_concat = torch.cat([th_pv, sp_pv_target], dim=1) # (B, 32+32) -> (B, 64)\n",
    "        \n",
    "        # --- 5. Rating Prediction ---\n",
    "        rating_pred = (user_pref_vec_concat * pfv).sum(dim=1)\n",
    "        \n",
    "        return rating_pred\n",
    "\n",
    "\n",
    "# 2. Load the BPE-LLM Encoder (from Cell 4)\n",
    "try:\n",
    "    if 'encoder' not in globals() or not isinstance(encoder, BpeLlmReviewEncoder):\n",
    "        print(\"Re-defining BpeLlmReviewEncoder class...\")\n",
    "        class BpeLlmReviewEncoder:\n",
    "            def __init__(self, model_name='distilbert-base-uncased', batch_size=64):\n",
    "                print(\"Initializing BPE-LLM Review Encoder...\")\n",
    "                self.device = device\n",
    "                self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "                self.model = DistilBertModel.from_pretrained(model_name).to(self.device).eval()\n",
    "                self.batch_size = batch_size\n",
    "                print(f\"Encoder initialized on {self.device} with model {model_name}.\")\n",
    "\n",
    "            @torch.no_grad()\n",
    "            def encode(self, review_texts):\n",
    "                all_embeddings = []\n",
    "                for i in range(0, len(review_texts), self.batch_size):\n",
    "                    batch_texts = review_texts[i:i+self.batch_size]\n",
    "                    inputs = self.tokenizer(\n",
    "                        batch_texts, padding=True, truncation=True, return_tensors='pt', max_length=512\n",
    "                    ).to(self.device)\n",
    "                    outputs = self.model(**inputs)\n",
    "                    token_embeddings = outputs.last_hidden_state\n",
    "                    attention_mask = inputs['attention_mask']\n",
    "                    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                    sum_embeddings = torch.sum(token_embeddings * mask_expanded, 1)\n",
    "                    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "                    mean_pooled = sum_embeddings / sum_mask\n",
    "                    all_embeddings.append(mean_pooled.cpu().numpy())\n",
    "                return np.vstack(all_embeddings)\n",
    "        \n",
    "        encoder = BpeLlmReviewEncoder()\n",
    "    print(\"BPE-LLM Encoder is ready.\")\n",
    "\n",
    "    # 3. Load the Config (from your WINNING run)\n",
    "    class Config:\n",
    "        # File path for Cell 12\n",
    "        TARGET_DOMAIN_FILE = 'Digital_Music.jsonl'\n",
    "        \n",
    "        # Model Dims\n",
    "        EMBEDDING_DIM = 768\n",
    "        FEATURE_DIM = 64\n",
    "        SHARED_DIM = 32\n",
    "        SPECIFIC_DIM = 32\n",
    "        TOP_K_REVIEWS = 10\n",
    "        DROPOUT_RATE = 0.3\n",
    "        \n",
    "        # Encoder config\n",
    "        EMBEDDING_BATCH_SIZE = 64\n",
    "    \n",
    "    print(\"Configuration from the winning run is loaded.\")\n",
    "    \n",
    "    # 4. Initialize the model and load the saved weights\n",
    "    inference_model = RACRecLLM(Config).to(device)\n",
    "    \n",
    "    # --- This is your best model file from Run 4 ---\n",
    "    model_path = 'best_model_simple_tuned.pth' \n",
    "    \n",
    "    inference_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    inference_model.eval() # Set model to evaluation mode (turns off dropout)\n",
    "\n",
    "    print(f\"\\nSuccessfully loaded trained model from '{model_path}'!\")\n",
    "    print(\"The recommendation engine is ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please make sure all previous cells (especially 2, 4, 7 from Run 4) have been defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building music item catalog... This may take a few minutes.\n",
      "Loading Digital_Music.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Digital_Music.jsonl: 10019it [00:00, 100175.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Digital_Music.jsonl: 130434it [00:02, 63180.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 118645 unique music reviews to encode.\n",
      "Found 70511 unique music items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating item vectors: 70511it [00:04, 14488.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog built in 1445.69 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Build the Recommendation Catalog\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm # Make sure tqdm is imported for the progress bar\n",
    "\n",
    "# --- ADDED MISSING HELPER FUNCTION ---\n",
    "def load_jsonl(path):\n",
    "    \"\"\"A robust function to load a JSON Lines file.\"\"\"\n",
    "    data = []\n",
    "    print(f\"Loading {path}...\")\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            # Added tqdm for progress\n",
    "            for line in tqdm(f, desc=f\"Reading {os.path.basename(path)}\"):\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except (json.JSONDecodeError, KeyError):\n",
    "                    pass # Skip malformed lines\n",
    "        return pd.DataFrame(data)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: File not found at {path}\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None\n",
    "# --- END OF FIX ---\n",
    "\n",
    "\n",
    "# We need the _pad_reviews helper function\n",
    "def _pad_reviews(reviews_list, max_reviews, emb_dim, zero_pad_vector):\n",
    "    \"\"\"Pads or truncates a list of review embeddings to a fixed size.\"\"\"\n",
    "    reviews = reviews_list[-max_reviews:]\n",
    "    padded_reviews = reviews + [zero_pad_vector] * (max_reviews - len(reviews))\n",
    "    return np.array(padded_reviews, dtype=np.float32)\n",
    "\n",
    "def build_item_catalog(encoder, config):\n",
    "    \"\"\"\n",
    "    Loads all music reviews, encodes them, and creates an aggregated\n",
    "    vector for every unique music item.\n",
    "    \"\"\"\n",
    "    print(\"Building music item catalog... This may take a few minutes.\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Load the raw Digital Music dataset\n",
    "    # This line will now work\n",
    "    music_df = load_jsonl(Config.TARGET_DOMAIN_FILE) \n",
    "    if music_df is None:\n",
    "        return None # Stop if file wasn't found\n",
    "        \n",
    "    music_df = music_df.dropna(subset=['text', 'parent_asin'])\n",
    "    music_df['text'] = music_df['text'].astype(str)\n",
    "    \n",
    "    # 2. Find all unique reviews in the music dataset and encode them\n",
    "    unique_reviews = music_df['text'].drop_duplicates().tolist()\n",
    "    print(f\"Found {len(unique_reviews)} unique music reviews to encode.\")\n",
    "    music_review_embeddings = encoder.encode(unique_reviews)\n",
    "    \n",
    "    # 3. Create a mapping for music reviews\n",
    "    music_embedding_map = {text: emb for text, emb in zip(unique_reviews, music_review_embeddings)}\n",
    "    music_df['embedding'] = music_df['text'].map(music_embedding_map)\n",
    "    music_df = music_df.dropna(subset=['embedding'])\n",
    "    \n",
    "    # 4. Group reviews by item_id\n",
    "    item_reviews_grouped = music_df.groupby('parent_asin')['embedding'].apply(list)\n",
    "    print(f\"Found {len(item_reviews_grouped)} unique music items.\")\n",
    "    \n",
    "    # 5. Create the final catalog: {item_id: aggregated_vector}\n",
    "    item_catalog = {}\n",
    "    zero_pad = np.zeros(config.EMBEDDING_DIM, dtype=np.float32)\n",
    "    \n",
    "    for item_id, reviews in tqdm(item_reviews_grouped.items(), desc=\"Aggregating item vectors\"):\n",
    "        # Pad the reviews for the item\n",
    "        padded_item_reviews = _pad_reviews(reviews, config.TOP_K_REVIEWS, config.EMBEDDING_DIM, zero_pad)\n",
    "        \n",
    "        # Aggregate using simple mean pooling (to match our winning model)\n",
    "        item_tensor = torch.tensor(padded_item_reviews, dtype=torch.float32)\n",
    "        item_agg_vec = torch.sum(item_tensor, dim=0) / torch.clamp((item_tensor.sum(dim=-1) != 0).sum(dim=0).unsqueeze(0), min=1)\n",
    "        \n",
    "        # Store the final vector (on CPU to save GPU memory)\n",
    "        item_catalog[item_id] = item_agg_vec.cpu()\n",
    "\n",
    "    print(f\"Catalog built in {time.time() - start_time:.2f} seconds.\")\n",
    "    return item_catalog\n",
    "\n",
    "# --- Build the catalog ---\n",
    "item_catalog = build_item_catalog(encoder, Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- REAL-TIME COLD-START RECOMMENDATION ENGINE (AUC: 0.8466) ---\n",
      "I have pre-processed all 70511 music items.\n",
      "Give me a new user's movie reviews (their 'Netflix' history).\n",
      "I will give you a Top-10 list of 'Spotify' recommendations.\n",
      "Type 'quit' at any time to exit.\n",
      "========================================\n",
      "Encoding user's movie reviews...\n",
      "Scoring all 70511 music items in the catalog...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recommending: 100%|██████████| 70511/70511 [00:19<00:00, 3595.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "💬 TOP-10 MUSIC RECOMMENDATIONS FOR THIS USER:\n",
      "----------------------------------------\n",
      "  1. Item (ASIN): B01K8O1NEY (Predicted Rating: 3.75)\n",
      "  2. Item (ASIN): B01K8Q6W20 (Predicted Rating: 3.68)\n",
      "  3. Item (ASIN): B000024SNU (Predicted Rating: 3.67)\n",
      "  4. Item (ASIN): B004OMR7EM (Predicted Rating: 3.65)\n",
      "  5. Item (ASIN): B00CKZKWU6 (Predicted Rating: 3.65)\n",
      "  6. Item (ASIN): B013GVMA4I (Predicted Rating: 3.64)\n",
      "  7. Item (ASIN): B01G65CPSC (Predicted Rating: 3.61)\n",
      "  8. Item (ASIN): B007P8LJ5K (Predicted Rating: 3.59)\n",
      "  9. Item (ASIN): B00824I8M8 (Predicted Rating: 3.59)\n",
      "  10. Item (ASIN): B00004SOF4 (Predicted Rating: 3.56)\n",
      "----------------------------------------\n",
      "Encoding user's movie reviews...\n",
      "Scoring all 70511 music items in the catalog...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recommending: 100%|██████████| 70511/70511 [00:19<00:00, 3616.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "💬 TOP-10 MUSIC RECOMMENDATIONS FOR THIS USER:\n",
      "----------------------------------------\n",
      "  1. Item (ASIN): B01K8Q6W20 (Predicted Rating: 3.55)\n",
      "  2. Item (ASIN): B004OMR7EM (Predicted Rating: 3.52)\n",
      "  3. Item (ASIN): B007P8LJ5K (Predicted Rating: 3.51)\n",
      "  4. Item (ASIN): B00824I8M8 (Predicted Rating: 3.51)\n",
      "  5. Item (ASIN): B00CKZKWU6 (Predicted Rating: 3.49)\n",
      "  6. Item (ASIN): B000024SNU (Predicted Rating: 3.47)\n",
      "  7. Item (ASIN): B006VFOM2K (Predicted Rating: 3.47)\n",
      "  8. Item (ASIN): B01K8O1NEY (Predicted Rating: 3.47)\n",
      "  9. Item (ASIN): B013GVMA4I (Predicted Rating: 3.46)\n",
      "  10. Item (ASIN): B00413ELMA (Predicted Rating: 3.41)\n",
      "----------------------------------------\n",
      "\n",
      "[Error] Please provide at least one movie review.\n",
      "\n",
      "[Error] Please provide at least one movie review.\n",
      "\n",
      "[Error] Please provide at least one movie review.\n",
      "\n",
      "[Error] Please provide at least one movie review.\n",
      "\n",
      "[Error] Please provide at least one movie review.\n",
      "\n",
      "[Error] Please provide at least one movie review.\n",
      "\n",
      "[Error] Please provide at least one movie review.\n",
      "\n",
      "[Error] Please provide at least one movie review.\n",
      "\n",
      "[Error] Please provide at least one movie review.\n",
      "\n",
      "[Error] Please provide at least one movie review.\n",
      "\n",
      "[Error] Please provide at least one movie review.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: The Final Recommendation Demo\n",
    "\n",
    "def get_recommendations(user_movie_reviews, model, encoder, item_catalog, config):\n",
    "    \"\"\"\n",
    "    Generates a Top-10 list of music recommendations for a cold-start user.\n",
    "    \"\"\"\n",
    "    print(\"Encoding user's movie reviews...\")\n",
    "    # 1. Encode the user's source (movie) reviews\n",
    "    user_source_embs = list(encoder.encode(user_movie_reviews))\n",
    "    \n",
    "    # 2. Pad them and create the user's source vector\n",
    "    zero_pad = np.zeros(config.EMBEDDING_DIM, dtype=np.float32)\n",
    "    usr_src_pad = _pad_reviews(user_source_embs, config.TOP_K_REVIEWS, config.EMBEDDING_DIM, zero_pad)\n",
    "    usr_src_tensor = torch.tensor(usr_src_pad, dtype=torch.float32).unsqueeze(0).to(device) # (1, K, E)\n",
    "\n",
    "    # 3. Create the zero vector for the cold-start target domain\n",
    "    usr_tgt_tensor = torch.zeros_like(usr_src_tensor).to(device)\n",
    "    \n",
    "    # 4. Aggregate the user vectors (matching the logic in our model)\n",
    "    uv_source = torch.sum(usr_src_tensor, dim=1) / torch.clamp((usr_src_tensor.sum(dim=-1) != 0).sum(dim=1).unsqueeze(1), min=1)\n",
    "    uv_target = torch.zeros_like(uv_source) # It's all zeros\n",
    "    \n",
    "    print(f\"Scoring all {len(item_catalog)} music items in the catalog...\")\n",
    "    predictions = {}\n",
    "    \n",
    "    # 5. Loop through the catalog and predict a rating for each item\n",
    "    with torch.no_grad():\n",
    "        for item_id, item_agg_vec in tqdm(item_catalog.items(), desc=\"Recommending\"):\n",
    "            iv_target = item_agg_vec.unsqueeze(0).to(device) # (1, E)\n",
    "\n",
    "            # 6. Use the model's forward pass to get a rating\n",
    "            rating_pred = model.forward_for_recommendation(uv_source, uv_target, iv_target)\n",
    "            predictions[item_id] = rating_pred.item()\n",
    "            \n",
    "    # 7. Sort the predictions and get the Top 10\n",
    "    sorted_recommendations = sorted(predictions.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    return sorted_recommendations[:10]\n",
    "\n",
    "# --- The Interactive Loop ---\n",
    "# We're referencing the AUC from your winning run here!\n",
    "print(f\"--- REAL-TIME COLD-START RECOMMENDATION ENGINE (AUC: 0.8466) ---\")\n",
    "print(f\"I have pre-processed all {len(item_catalog)} music items.\")\n",
    "print(\"Give me a new user's movie reviews (their 'Netflix' history).\")\n",
    "print(\"I will give you a Top-10 list of 'Spotify' recommendations.\")\n",
    "print(\"Type 'quit' at any time to exit.\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "while True:\n",
    "    user_input_movies = input(\"\\n🎬 USER'S MOVIE REVIEWS (separate with ';'): \\n\")\n",
    "    if user_input_movies.lower() == 'quit':\n",
    "        break\n",
    "        \n",
    "    movie_reviews_list = [r.strip() for r in user_input_movies.split(';') if r.strip()]\n",
    "\n",
    "    if not movie_reviews_list:\n",
    "        print(\"\\n[Error] Please provide at least one movie review.\")\n",
    "        continue\n",
    "\n",
    "    # Get the recommendations\n",
    "    try:\n",
    "        recommendations = get_recommendations(\n",
    "            movie_reviews_list, \n",
    "            inference_model, \n",
    "            encoder, \n",
    "            item_catalog, \n",
    "            Config\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"💬 TOP-10 MUSIC RECOMMENDATIONS FOR THIS USER:\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, (item_id, rating) in enumerate(recommendations):\n",
    "            print(f\"  {i+1}. Item (ASIN): {item_id} (Predicted Rating: {rating:.2f})\")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during prediction: {e}\")\n",
    "\n",
    "print(\"\\n--- Demo finished. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: The Interactive \"Rating Prediction\" Demo\n",
    "\n",
    "print(\"--- Cold-Start Rating Prediction Tester ---\")\n",
    "print(f\"I am your best trained model (AUC: 0.8466).\")\n",
    "print(\"Give me a user's movie reviews (source history).\")\n",
    "print(\"Then, give me a specific music item's reviews (target item).\")\n",
    "print(\"I will predict the single rating that user would give that item.\")\n",
    "print(\"Type 'quit' at any time to exit.\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "while True:\n",
    "    # 1. Get user's movie reviews\n",
    "    user_input_movies = input(\"\\n🎬 USER'S MOVIE REVIEWS (separate with ';'): \\n\")\n",
    "    if user_input_movies.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    # 2. Get item's music reviews\n",
    "    user_input_music_item = input(\"\\n🎵 MUSIC ITEM'S REVIEWS (separate with ';'): \\n\")\n",
    "    if user_input_music_item.lower() == 'quit':\n",
    "        break\n",
    "        \n",
    "    # Split the input strings into lists\n",
    "    movie_reviews_list = [r.strip() for r in user_input_movies.split(';') if r.strip()]\n",
    "    music_item_reviews_list = [r.strip() for r in user_input_music_item.split(';') if r.strip()]\n",
    "\n",
    "    if not movie_reviews_list:\n",
    "        print(\"\\n[Error] Please provide at least one movie review.\")\n",
    "        continue\n",
    "    if not music_item_reviews_list:\n",
    "        print(\"\\n[Error] Please provide at least one review for the music item.\")\n",
    "        continue\n",
    "\n",
    "    # 3. Get the prediction\n",
    "    try:\n",
    "        rating = predict_cold_start_rating(\n",
    "            movie_reviews_list, \n",
    "            music_item_reviews_list, \n",
    "            inference_model, \n",
    "            encoder, \n",
    "            Config\n",
    "        )\n",
    "        \n",
    "        # 4. Print the result\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"💬 MODEL PREDICTION:\")\n",
    "        print(f\"   Based on the user's movie taste, I predict they would rate this music item:\")\n",
    "        print(f\"\\n   >>>>> {rating:.2f} out of 5 stars <<<<<\")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during prediction: {e}\")\n",
    "\n",
    "print(\"\\n--- Demo finished. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
